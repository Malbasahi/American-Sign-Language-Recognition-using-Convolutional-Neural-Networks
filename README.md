# American-Sign-Language-Recognition-using-Convolutional-Neural-Networks
Real-time American Sign Language Recognition using Convolutional Neural Networks: Designing and Evaluating a High-Accuracy Model
The project's importance lies in providing a dependable and effective communication tool as a way to enhance the quality of life for those who have hearing impairments. The project's accomplishments show the promise of machine learning methods for enhancing accessibility and inclusion for those with impairments.
The system recognizes the hand in the video stream and extracts the hand region from the image using the CVZone framework. The preprocessed extracted hand area is then set into a CNN-based classification algorithm. A collection of 2201 images from 26 classes of the English alphabet was used to train the model. The dataset was gathered by capturing a video of a user making actions in the language. The proposed system obtained 99.27% accuracy with a 2.7% loss. The system's effectiveness was assessed using a live video feed, and the findings demonstrate that it is capable of real-time sign language gesture recognition. The system can be improved by adding more complicated hand gestures and broadening the dataset to include additional classes and variants of hand movements.
# Requirements
Python==3.9\ Opencv-python\ CVzone\ NumPy==1.23.4
